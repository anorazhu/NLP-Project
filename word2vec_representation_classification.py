# -*- coding: utf-8 -*-
"""word2vec representation classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QJlNLa2jehPogCbd1QKhHFoANBX2bzAJ

1. Preprocessing: combine skills, education, cerifications, and job role into a singular, sanitized text field
"""

import nltk
nltk.download('punkt_tab')
import ssl
import re
import matplotlib.pyplot as plt
import numpy as np
import gensim
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
import pandas as pd

#import dataset
from google.colab import files
uploaded = files.upload()

#get data
data = open('data.csv').read()
alltokens = nltk.word_tokenize(data)

#see number of tokens, types in text
tokencount = len(alltokens)
print("Number of tokens:", tokencount)

typecount = len(set(alltokens))
print("Number of types:", typecount)

#type-token ratio -- unique words/total words
#should be low, because text is not super varied
typetokenratio = typecount/tokencount
print("Type-Token Ratio:", typetokenratio)

#10 most common tokens
fdist = nltk.FreqDist(alltokens)
print(fdist.most_common(10))

#combine text columns into one text field
def combine_text(row):
  fields = [row['Skills'], row['Education'], row['Certifications'], row['Job Role']]
  return ''.join(str(fields).strip())

data = pd.read_csv('data.csv')
data['text'] = data.apply(combine_text, axis=1)
print(data['text'])

#use preloaded cleansing model
from gensim.utils import simple_preprocess
data['tokens'] = data['text'].apply(simple_preprocess)
print(data['tokens'])

#load pretrained word2vec model (word2vec-google-news-300)

import gensim.downloader as api
word2vec = api.load('word2vec-google-news-300')

#convert each row to a vector, average all the word vectors in the token list

#average across all word vectors
#if none of the words in the model, return 0 vector
def get_doc_vector(tokens, model):
  vectors = [model[token] for token in tokens if token in model]
  if not vectors:
    return np.zeros(model.vector_size)
  return np.mean(vectors, axis=0)

data['vectors'] = data['tokens'].apply(lambda x: get_doc_vector(x, word2vec))
print(data['vectors'])

"""Now that we have processed our data and generated vectors by averaging using a pretrained word2vec model, we can start our evaluation.

1. Statistical Evaluations - Naive Bayes
"""

#train/test split, structure input

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.preprocessing import LabelEncoder

data['vectors'].tolist(), data['Job Role']

#structure
#x: matrix of rows -- num_samples x vector_dim
x = np.vstack(data['vectors'].values)

#y: target labels (hire = 1, reject = 0)
le = LabelEncoder()
y = le.fit_transform(data['Recruiter Decision'])

#split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

#use the NB classifer
naive_bayes = GaussianNB()
naive_bayes.fit(x_train, y_train)

prediction = naive_bayes.predict(x_test)

#evaluate!
print("Accuracy:", accuracy_score(y_test, prediction))
print("Precision:",precision_score(y_test, prediction))
print("Recall:",recall_score(y_test, prediction))
print("F1 Score:",f1_score(y_test, prediction))
print("Classification Report:\n\n", classification_report(y_test, prediction, target_names=le.classes_, zero_division=0))

"""2. Stastitical Evaluations - SVM"""

#use the same setup
from sklearn.svm import SVC

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

#train SVM classifier
#balance class weight to adjust weight of each class inversely proportional
#to frequency -- avoids "Reject" class being overlooked & having all fields = 0
svm = SVC(kernel = 'linear', C = 1.0, class_weight = 'balanced',random_state = 42)
svm.fit(x_train, y_train)

#predict & eval
prediction = svm.predict(x_test)

print("Accuracy:", accuracy_score(y_test, prediction))
print("Precision:",precision_score(y_test, prediction))
print("Recall:",recall_score(y_test, prediction))
print("F1 Score:",f1_score(y_test, prediction))
print("Classification Report:\n\n", classification_report(y_test, prediction, target_names=le.classes_, zero_division=0))

"""3. Neural Classifications - Feedforward Neural Network (FFNN)"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight

#define the FFNN model using standard numbers, binary output
ffnn = Sequential([
    Dense(128, activation='relu', input_shape=(300,)),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

#compile model
ffnn.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])

#train classifier
  #balance weights to give more weight to the minority to get a more
  #comprehensive result
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))

ffnn.fit(x_train, y_train, epochs=20, batch_size=32, validation_split = 0.2, validation_data=(x_test, y_test), class_weight = class_weight_dict)

#predict & eval!
probability = ffnn.predict(x_test)
prediction = [1 if p > 0.5 else 0 for p in probability]

#print results
print("Accuracy:", accuracy_score(y_test, prediction))
print("Precision:",precision_score(y_test, prediction))
print("Recall:",recall_score(y_test, prediction))
print("F1 Score:",f1_score(y_test, prediction))
print("Classification Report:\n\n", classification_report(y_test, prediction, target_names=le.classes_, zero_division=0))

"""2. Neural Classifiers -- Convolutonal Neural Network (CNN)"""

from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Flatten
from tensorflow.keras.preprocessing.sequence import pad_sequences

#regularize sequence length
max_length = 100
dimension = 300 #from word2vec

#create list of word vectors from tokenized text
#return zero vector if necessary
def tokens_to_matrix(tokens, word2vec):
  vectors = [word2vec[token] for token in tokens if token in word2vec]
  if vectors:
    return np.array(vectors)
  else:
    return np.zeros(1, dimension)

#create 2D array for each row
row_matrix = []
for tokens in data['tokens']:
  matrix = [word2vec[token] for token in tokens if token in word2vec]
  if not matrix:
    matrix = np.zeros(dimension)
  regularized = pad_sequences([matrix], maxlen=max_length, dtype='float32', padding='post', truncating='post')[0]
  row_matrix.append(regularized)

x_cnn = np.array(row_matrix)

#split
x_train_cnn, x_test_cnn, y_train_cnn, y_test_cnn = train_test_split(x_cnn, y, test_size=0.2, random_state=42)

cnn = Sequential([
    Conv1D(128, kernel_size=5, activation='relu', input_shape=(max_length, dimension)),
    GlobalMaxPooling1D(),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

#balance weights, train classifier
class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_cnn), y=y_train_cnn)
class_weight_dict = dict(enumerate(class_weights))

cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

cnn.fit(x_train_cnn, y_train_cnn, epochs=20, batch_size=32, validation_split=0.2, class_weight=class_weight_dict)

#predict & eval
prediction = (cnn.predict(x_test_cnn) > 0.5).astype(int)


#print results
print("Accuracy:", accuracy_score(y_test_cnn, prediction))
print("Precision:",precision_score(y_test_cnn, prediction))
print("Recall:",recall_score(y_test_cnn, prediction))
print("F1 Score:",f1_score(y_test_cnn, prediction))
print("Classification Report:\n\n", classification_report(y_test_cnn, prediction, target_names=le.classes_, zero_division=0))